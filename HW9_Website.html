<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Ammar Plumber, Elaina Lin, Kim Nguyen, Meghan Aines, Ryan Karbowicz" />


<title>Homework 9 - Alligators</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/simplex.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 41px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h2 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h3 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h4 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h5 {
  padding-top: 46px;
  margin-top: -46px;
}
.section h6 {
  padding-top: 46px;
  margin-top: -46px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Ryan Karbowicz</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="project.html">Project</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="HW9_Website.html">HW9</a>
</li>
<li>
  <a href="HW10_Website.html">HW10</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Homework 9 - Alligators</h1>
<h4 class="author">Ammar Plumber, Elaina Lin, Kim Nguyen, Meghan Aines, Ryan Karbowicz</h4>
<h4 class="date">4/13/2021</h4>

</div>


<div id="i.-introduction" class="section level1">
<h1>I. Introduction</h1>
<p>We will be examining the difference in tweet communications between TikTok and Facebook. These are two popular social media platforms but with very different target audiences. Thus, the two brands may differ in their communication styles and language. We set out to identify the particular ways in which they differ and to build a model that can attribute each tweet to the correct user.</p>
</div>
<div id="ii.-methodology" class="section level1">
<h1>II. Methodology</h1>
<p>First, after getting tweets using the Twitter API and R package rtweet, we use basic tools of data exploration to transform, visualize, and examine different features of the datasets, such as source, time, length, and particular contents (e.g., picture/links) of the tweets. We produce bar charts to visualize the most popular words used by each twitter account, as well as the most popular sentiments associated with tweets that each account produces. A word cloud also helps paint a clearer picture of each company’s most commonly used words.</p>
<p>Second, we transform the datasets into tidytext format for sentiment analysis. The two lexicons that we use are NRC and AFINN.</p>
<p>Finally, we train four different models to predict if a tweet was posted by either Facebook or TikTok. The inputs of these models are as follows:</p>
<ul>
<li><p>length of the tweet</p></li>
<li><p>the number of times each of the twenty most common words from each account appear</p></li>
<li><p>presence of pictures</p></li>
<li><p>sentiment words, which reflect anger, anticipation, disgust, negative, postive, trust, joy, surprise, fear and sadness</p></li>
<li><p>positive/negative valence, estimated by averaging AFINN scores over each tweet.</p></li>
</ul>
<p>The first model is a Simple Decision Tree, the second model is a Bagging Model, the third model is a Random Forest and the fourth model is a Gradient Boosting Model.</p>
<p>We report the residual sum of squares on the training and test sets to determine which models have the smallest differences between the predicted tweeter and actual tweeter. We also show confusion matrices to determine the predictive efficacy of the four models.</p>
</div>
<div id="iii.-setup-and-preliminary-analysis" class="section level1">
<h1>III. Setup and Preliminary Analysis</h1>
<p>First, we import all non-base packages to be used in this analysis.</p>
<pre class="r"><code>library(rtweet)
library(tidyverse)
library(lubridate)
library(scales)
library(tidytext)
library(wordcloud)
library(textdata)

library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(rpart.plot)
library(ipred)       # for fitting bagged decision trees
library(ranger)
library(gbm)
library(vip)

library(kableExtra)</code></pre>
<p>Now, we import the tweets that we pulled using the get_timeline() function and saved to a CSV file. There are ~3200 tweets from each user in our dataset.</p>
<pre class="r"><code># Run these two lines to get the tweets 
# and then save them as a csv for future use
# tiktok &lt;- get_timeline(&quot;tiktok_us&quot;, n=3200)
# tiktok %&gt;% write_as_csv(&#39;tiktok.csv&#39;)
# 
# facebook &lt;- get_timeline(&quot;Facebook&quot;, n=3200)
# facebook %&gt;% write_as_csv(&quot;facebook.csv&quot;)

tiktok &lt;-
  read_csv(&#39;tiktok.csv&#39;) %&gt;% 
  select(status_id, source, text, created_at) %&gt;% 
  as.data.frame()

facebook &lt;-
  read_csv(&#39;facebook.csv&#39;) %&gt;% 
  select(status_id, source, text, created_at)

nrc &lt;- read_rds(&quot;nrc.rds&quot;)

facebook %&gt;% head()</code></pre>
<pre><code>## # A tibble: 6 x 4
##   status_id        source      text                                                                  created_at         
##   &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;                                                                 &lt;dttm&gt;             
## 1 x13820200803434… Twitter We… &quot;Ramadan Mubarak &lt;U+0001F319&gt;\n \nThis #MonthofGood, check out all t… 2021-04-13 17:17:18
## 2 x13817344290186… Khoros CX   &quot;@MeenalK1 Hi Meenal. Do you have the reference numbers for your sub… 2021-04-12 22:22:13
## 3 x13817333826320… Khoros CX   &quot;@Afrojalipro Thanks for updating us, Afroj! We&#39;re so happy to hear … 2021-04-12 22:18:04
## 4 x13817326683881… Khoros CX   &quot;@CallandManning Hi Calland. If you do not have access to the phone … 2021-04-12 22:15:14
## 5 x13817113768763… Khoros CX   &quot;@BHARTINANDAN4 Hello! Please visit this Help Center article if you … 2021-04-12 20:50:37
## 6 x13817105484761… Khoros CX   &quot;@weathermatt22 Hi Matt. Please visit our Help Center to report an i… 2021-04-12 20:47:20</code></pre>
<p>Now, for each user, we produce a line chart showing the percent of all tweets from each source by hour.</p>
<pre class="r"><code>facebook %&gt;%
  count(source, hour = hour(with_tz(created_at, &quot;EST&quot;))) %&gt;%
  mutate(percent = n/sum(n)) %&gt;%
  ggplot(aes(x = hour, y = percent, color = source)) +
  labs(x = &quot;Hour of day (EST)&quot;, y = &quot;% of tweets&quot;, color = &quot;&quot;) + 
  scale_y_continuous(labels = percent_format()) +
  geom_line() +
  ggtitle(&#39;Facebook Source Breakdown by Hour&#39;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>tiktok %&gt;%
  count(source, hour = hour(with_tz(created_at, &quot;EST&quot;))) %&gt;%
  mutate(percent = n/sum(n)) %&gt;%
  ggplot(aes(x = hour, y = percent, color = source)) +
  labs(x = &quot;Hour of day (EST)&quot;, y = &quot;% of tweets&quot;, color = &quot;&quot;) + 
  scale_y_continuous(labels = percent_format()) +
  geom_line() +
  ggtitle(&#39;Tiktok Source Breakdown by Hour&#39;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<p>We see that the vast majority of Facebook’s tweets are put out using Khoros Publishing between the hours of 10 AM and 8 PM. TikTok publishes most of its tweets through the Twitter Web App and Fan Experiences Platform—usually between 10 AM and 8 PM, like Facebook.</p>
<p>We want to see if both users’ tweets tend to differ in length, so we create a histogram for each user.</p>
<pre class="r"><code>fb_wordcounts &lt;- 
  facebook %&gt;%
  mutate(tweetLength = str_length(text)) %&gt;% 
  filter(tweetLength &lt; 500)

tiktok_wordcounts &lt;- 
  tiktok %&gt;%
  mutate(tweetLength = str_length(text)) %&gt;% 
  filter(tweetLength &lt; 500)

writeLines(c(paste0(&quot;Facebook Mean Tweet Length: &quot;, 
                  mean(fb_wordcounts$tweetLength)), 
           paste0(&quot;TikTok Mean Tweet Length: &quot;, 
                  mean(tiktok_wordcounts$tweetLength))))</code></pre>
<pre><code>## Facebook Mean Tweet Length: 163.289555972483
## TikTok Mean Tweet Length: 112.557921102066</code></pre>
<pre class="r"><code>hist(tiktok_wordcounts$tweetLength, main = &quot;TikTok - Histogram of Tweet Lengths&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>hist(fb_wordcounts$tweetLength, main = &quot;Facebook - Histogram of Tweet Lengths&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<p>As we see, TikTok’s tweet lengths are right-skewed, with most tweets being around 100 words long. Facebook, on the other hand, seems to post longer tweets, with a more normal distribution centered around 150 words long. Tweet length seems like a useful feature to include in our predictive model.</p>
<p>Next, we look at whether there is a difference in the share of Tweets that include pictures.</p>
<pre class="r"><code>fb_picture_counts &lt;- 
  facebook %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  count(picture = ifelse(str_detect(text, &quot;t.co&quot;),
                         &quot;Picture/link&quot;, &quot;No picture/link&quot;))

fb_picture_counts &lt;- 
  fb_picture_counts %&gt;% 
  mutate(prop = n / sum(fb_picture_counts$n) *100)

tiktok_picture_counts &lt;- 
  tiktok %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  count(picture = ifelse(str_detect(text, &quot;t.co&quot;),
                         &quot;Picture/link&quot;, &quot;No picture/link&quot;))

tiktok_picture_counts &lt;- 
  tiktok_picture_counts %&gt;% 
  mutate(prop = n / sum(tiktok_picture_counts$n) *100)

fb_picture_counts %&gt;% 
  ggplot(aes(x = &quot;&quot;, y = n, fill = picture)) +
  geom_bar(width = 1, stat = &quot;identity&quot;) +
  coord_polar(&quot;y&quot;, start=0) +
  theme_void() +
  geom_text(aes(label = paste0(round(prop,2), &quot;%&quot;)), 
            position = position_stack(vjust = 0.5), size = 4) + 
  ggtitle(&quot;Percent of Facebook Tweets with Picture/link and Without&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>tiktok_picture_counts %&gt;% 
  ggplot(aes(x = &quot;&quot;, y = n, fill = picture)) +
  geom_bar(width = 1, stat = &quot;identity&quot;) +
  coord_polar(&quot;y&quot;, start=0) +
  theme_void() +
  geom_text(aes(label = paste0(round(prop,2), &quot;%&quot;)), 
            position = position_stack(vjust = 0.5), size = 4) + 
  ggtitle(&quot;Percent of TikTok Tweets with Picture/link and Without&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p>~86% of Facebook’s tweets contain pictures/links, while only ~52% of TikTok’s tweets contain pictures/links. This could be another useful predictor to include in our model.</p>
</div>
<div id="iv.-sentiment-analysis" class="section level1">
<h1>IV. Sentiment Analysis</h1>
<p>Now, we split the tweets into tokens so that we can perform sentiment analysis on them.</p>
<pre class="r"><code>reg &lt;- &quot;([^A-Za-z\\d#@&#39;]|&#39;(?![A-Za-z\\d#@]))&quot;

# Unnest the text strings into a data frame of words
fb_words &lt;- 
  facebook %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(text = str_replace_all(text, 
                                &quot;https://t.co/[A-Za-z\\d]+|&amp;amp;&quot;, 
                                &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, 
                token = &quot;regex&quot;, 
                pattern = reg) %&gt;%
  filter(!word %in% stop_words$word,
         str_detect(word, &quot;[a-z]&quot;))

tiktok_words &lt;- 
  tiktok %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(text = str_replace_all(text, 
                                &quot;https://t.co/[A-Za-z\\d]+|&amp;amp;&quot;, 
                                &quot;&quot;)) %&gt;%
  unnest_tokens(word, text, 
                token = &quot;regex&quot;, 
                pattern = reg) %&gt;%
  filter(!word %in% stop_words$word,
         str_detect(word, &quot;[a-z]&quot;))

# Inspect the first six rows of tweet_words
head(fb_words)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   status_id            source          created_at          word        
##   &lt;chr&gt;                &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt;       
## 1 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 ramadan     
## 2 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 mubarak     
## 3 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 0001f319    
## 4 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 #monthofgood
## 5 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 check       
## 6 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 kindness</code></pre>
<p>We produce two horizontal bar graphs that show the most common words along with a word cloud for each user.</p>
<pre class="r"><code>fb_most_common &lt;- 
  fb_words %&gt;%
  count(word, sort = TRUE) %&gt;%
  head(20) %&gt;%
  mutate(word = reorder(word, n))

fb_most_common %&gt;%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = &quot;identity&quot;) +
  ylab(&quot;Occurrences&quot;) +
  coord_flip() + 
  ggtitle(&quot;Facebook Word Frequency&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>tiktok_most_common &lt;- 
  tiktok_words %&gt;%
  count(word, sort = TRUE) %&gt;%
  head(20) %&gt;%
  mutate(word = reorder(word, n))

tiktok_most_common %&gt;%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = &quot;identity&quot;) +
  ylab(&quot;Occurrences&quot;) +
  coord_flip() +
  ggtitle(&quot;TikTok Word Frequency&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code>facebook_cloud &lt;- 
  fb_words  %&gt;% 
  count(word) %&gt;% 
  arrange(-n)

wordcloud(facebook_cloud$word, 
          facebook_cloud$n, max.words = 200, 
          colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, 
                     &quot;#FF0099&quot;, &quot;#6600CC&quot;, 
                     &quot;green&quot;, &quot;orange&quot;, 
                     &quot;blue&quot;, &quot;brown&quot;))</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-8-3.png" width="672" /></p>
<pre class="r"><code>tiktok_cloud &lt;- 
  tiktok_words  %&gt;% 
  count(word) %&gt;% 
  arrange(-n)

wordcloud(tiktok_cloud$word, 
          tiktok_cloud$n, max.words = 200, 
          colors = c(&quot;#00B2FF&quot;, &quot;red&quot;, 
                     &quot;#FF0099&quot;, &quot;#6600CC&quot;, 
                     &quot;green&quot;, &quot;orange&quot;, 
                     &quot;blue&quot;, &quot;brown&quot;))</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-8-4.png" width="672" /></p>
<p>The most common word that Facebook uses is “kn,” and it’s unclear what this means. We did a quick search and found that Facebook signs off a lot of its replies with “-KN.” See here: <a href="https://twitter.com/Facebook/status/1185298970832244736" class="uri">https://twitter.com/Facebook/status/1185298970832244736</a></p>
<p>Facebook frequently uses neutral and security-/support-related words: “account,” “report,” “secure,” “experiencing,” etc.</p>
<p>TikTok uses a lot more anticipatory words like “tomorrow,” “prizes,” “nomination,” “winner.” TikTok’s Twitter account actually has a lot of sweepstakes.</p>
<p>Both accounts, of course, reference their own company names frequently.</p>
<p>We will use the number of times these forty words appear as predictors in our model.</p>
<p>We now join the NRC Word-Emotion Association Lexicon to our data, which will allow us to identify words associated with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).</p>
<pre class="r"><code>fb_sentiment &lt;-
  inner_join(fb_words, nrc, by = &quot;word&quot;)

tiktok_sentiment &lt;-
  inner_join(tiktok_words, nrc, by = &quot;word&quot;)

fb_sentiment %&gt;% head()</code></pre>
<pre><code>## # A tibble: 6 x 5
##   status_id            source          created_at          word      sentiment   
##   &lt;chr&gt;                &lt;chr&gt;           &lt;dttm&gt;              &lt;chr&gt;     &lt;chr&gt;       
## 1 x1382020080343470082 Twitter Web App 2021-04-13 17:17:18 kindness  positive    
## 2 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     anticipation
## 3 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     joy         
## 4 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     positive    
## 5 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 happy     trust       
## 6 x1381733382632001536 Khoros CX       2021-04-12 22:18:04 wonderful joy</code></pre>
<p>Here, we compare Facebook’s and TikTok’s sentiments.</p>
<pre class="r"><code>fb_sentiment_analysis &lt;- 
  fb_sentiment %&gt;% 
  count(word, sentiment) %&gt;% 
  group_by(sentiment)

fb_sentiment_analysis %&gt;%  
  top_n(15) %&gt;% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = &quot;identity&quot;) +
  coord_flip() +
  ylab(&quot;Frequency&quot;) +
  xlab(&quot;Sentiment&quot;) +
  labs(title=&quot;Facebook Sentiment&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>tiktok_sentiment_analysis &lt;- 
  tiktok_sentiment %&gt;% 
  count(word, sentiment) %&gt;% 
  group_by(sentiment)

tiktok_sentiment_analysis %&gt;%  
  top_n(15) %&gt;% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = &quot;identity&quot;) +
  coord_flip() +
  ylab(&quot;Frequency&quot;) +
  xlab(&quot;Sentiment&quot;) +
  labs(title=&quot;TikTok Sentiment&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<p>It looks like Facebook’s tweets use more trust words while TikTok uses more words that reflect anticipation. We now show specifically which words are conveying each of these observed sentiments.</p>
<pre class="r"><code>fb_sentiment_analysis %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %&gt;% top_n(10) -&gt; fb_sentiment_analysis2

ggplot(fb_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = &quot;free&quot;)+ 
  geom_bar(stat =&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y=&quot;count&quot;, title=&quot;Facebook Sentiment Words&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>tiktok_sentiment_analysis %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %&gt;% top_n(10) -&gt; tiktok_sentiment_analysis2

ggplot(tiktok_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = &quot;free&quot;)+ 
  geom_bar(stat =&quot;identity&quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y=&quot;count&quot;, title=&quot;Tik Tok Sentiment Words&quot;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<p>Next, we examine texts on Facebook and Tiktok to see their positive-negative score by using the AFINN sentiment lexicon, a list of English terms manually rated for valence with an integer between -5 (negative) and +5 (positive) by Finn Årup Nielsen between 2009 and 2011.</p>
<p>We use this lexicon to compute mean positivity scores for all words tweeted by each user.</p>
<pre class="r"><code># run this to get afinn lexicon and save it as a csv
# get_sentiments (&quot;afinn&quot;) -&gt; afinn
#
#afinn %&gt;% write_as_csv(&quot;afinn.csv&quot;)

afinn &lt;- read_csv(&#39;afinn.csv&#39;)

fb_afinn &lt;-    
 inner_join(fb_words, 
            afinn, 
            by = &quot;word&quot;)

tiktok_afinn &lt;-    
 inner_join(tiktok_words, 
            afinn, 
            by = &quot;word&quot;)

fb_mean_afinn &lt;- 
  fb_afinn %&gt;% 
  summarise(mean_fb_afinn = mean(value))

tiktok_mean_afinn &lt;- 
  tiktok_afinn %&gt;% 
  summarise(mean_tt_afinn = mean(value))

cat(paste0(&quot;Average AFINN scores for all words by user\n&quot;,
           &quot;\nFacebook: &quot;, round(fb_mean_afinn, 3), 
           &quot;\nTikTok: &quot;, round(tiktok_mean_afinn, 3)))</code></pre>
<pre><code>## Average AFINN scores for all words by user
## 
## Facebook: 0.785
## TikTok: 1.704</code></pre>
<p>Facebook’s mean AFINN value is 0.79 while TikTok’s mean AFINN value is 1.704. In general, words tweeted by Tiktok are more positive than those tweeted by Facebook.</p>
</div>
<div id="v.-training-predictive-models" class="section level1">
<h1>V. Training Predictive Models</h1>
<p>Here, using the text of a tweet, we attempt to predict the user who tweeted it.</p>
<p>The features we extracted are tweet length, the number of times each of the twenty most common words from each account appear, the presence of a picture/link, number of words for each sentiment, and mean AFINN score per tweet.</p>
<p>TikTok is encoded as 1, and Facebook is encoded as 0.</p>
<p>First, we prepare the data for training and produce a simple decision tree.</p>
<pre class="r"><code>fbcommon &lt;- 
  lapply(fb_most_common$word, as.character) %&gt;% 
  unlist()

tiktokcommon &lt;- 
  lapply(tiktok_most_common$word, as.character) %&gt;% 
  unlist()

commonwords &lt;- c(tiktokcommon, fbcommon)

fb_word_predict &lt;- 
  fb_words %&gt;% 
  filter(word %in% commonwords) %&gt;% 
  group_by(status_id) %&gt;% 
  count(word) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(id_cols = status_id, 
              names_from = word, 
              values_from = n,
              values_fill = 0)

tiktok_word_predict &lt;- 
  tiktok_words %&gt;% 
  filter(word %in% commonwords) %&gt;% 
  group_by(status_id) %&gt;% 
  count(word) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(id_cols = status_id, 
              names_from = word, 
              values_from = n,
              values_fill = 0)

fb_piclinks &lt;-
  facebook %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(picture_link = ifelse(str_detect(text, &quot;t.co&quot;),
                         1, 0)) %&gt;% 
  select(1,5)

tiktok_piclinks &lt;- 
  tiktok %&gt;%
  filter(!str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(picture_link = ifelse(str_detect(text, &quot;t.co&quot;),
                         1, 0)) %&gt;% 
  select(1,5)

fb_tweet_afinn &lt;- 
  fb_afinn %&gt;% 
  group_by(status_id) %&gt;% 
  summarize(afinn = mean(value))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>tiktok_tweet_afinn &lt;- 
  tiktok_afinn %&gt;% 
  group_by(status_id) %&gt;% 
  summarize(afinn = mean(value))</code></pre>
<pre><code>## `summarise()` ungrouping output (override with `.groups` argument)</code></pre>
<pre class="r"><code>fb_sentiment_counts &lt;- 
  fb_sentiment %&gt;% 
  group_by(status_id) %&gt;% 
  count(sentiment) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(id_cols = status_id, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)

tiktok_sentiment_counts &lt;- 
  tiktok_sentiment %&gt;% 
  group_by(status_id) %&gt;% 
  count(sentiment) %&gt;% 
  ungroup() %&gt;% 
  pivot_wider(id_cols = status_id, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)

tiktok_feature_selection &lt;- 
  tiktok_wordcounts %&gt;% 
  mutate(user = 1) %&gt;% 
  left_join(tiktok_sentiment_counts, 
            by=&quot;status_id&quot;) %&gt;% 
  left_join(tiktok_tweet_afinn,
            by=&quot;status_id&quot;) %&gt;% 
  left_join(tiktok_piclinks,
            by=&quot;status_id&quot;) %&gt;% 
  left_join(tiktok_word_predict,
            by = &quot;status_id&quot;)

facebook_feature_selection &lt;-
  fb_wordcounts %&gt;% 
  mutate(user = 0) %&gt;% 
  left_join(fb_sentiment_counts, 
            by=&quot;status_id&quot;) %&gt;% 
  left_join(fb_tweet_afinn,
            by=&quot;status_id&quot;) %&gt;% 
  left_join(fb_piclinks,
            by=&quot;status_id&quot;) %&gt;% 
  left_join(fb_word_predict,
            by = &quot;status_id&quot;)

both_users &lt;- 
  tiktok_feature_selection %&gt;% 
  dplyr::bind_rows(facebook_feature_selection) %&gt;%
  mutate_if(is.numeric,coalesce,0)

set.seed(123)
index &lt;- 
  createDataPartition(both_users$user,
                      p = 0.8, list = FALSE)

for_decisiontree &lt;-
  both_users %&gt;% select(-1,-2,-3,-4)

train &lt;- for_decisiontree[index, ]
test  &lt;- for_decisiontree[-index, ]

colnames(train) &lt;- make.names(colnames(train))
colnames(test) &lt;- make.names(colnames(test))

set.seed(123)
simple_model &lt;- rpart(user ~ ., 
                      data = train, method = &quot;class&quot;)
rpart.plot(simple_model, yesno = 2)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>It seems that the most dominant predictors in the simple model were the presence (or non-presence) of Facebook’s most common words, along with tweet length and the presence of the word “tiktok.” I suspect this will be similar for other models, though perhaps sentiment will play a role too.</p>
<p>We produce additional models using the bagging, random forests, and gradient boosting methods.</p>
<pre class="r"><code>set.seed(123)
bagging_model &lt;- train(
  user ~ .,
  data = train,
  method = &quot;treebag&quot;,
  trControl = trainControl(method = &quot;oob&quot;),
  keepX = T,
  nbagg = 100,
  importance = &quot;impurity&quot;,
  control = rpart.control(minsplit = 2, cp = 0)
)

n_features &lt;- length(setdiff(names(train), &quot;user&quot;))

train$user &lt;- as.factor(train$user)
rf_model &lt;- ranger(
  user ~ .,
  data = train,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = &quot;order&quot;,
  importance = &quot;permutation&quot;,
  seed = 123
)


set.seed(123)  # for reproducibility
train$user &lt;- as.numeric(train$user)-1
gbm_model &lt;- gbm(
  formula = user ~ .,
  data = train,
  distribution = &quot;gaussian&quot;,  # SSE loss function
  n.trees = 1000,
  shrinkage = 0.05,
  interaction.depth = 5,
  n.minobsinnode = 4,
  cv.folds = 10
)</code></pre>
<p>We also display four variable importance plots to see which variables each model identified as significant.</p>
<pre class="r"><code>vip(simple_model, num_features = 25) + 
  ggtitle(&#39;Simple Decision Tree - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>vip(bagging_model, num_features = 25) + 
  ggtitle(&#39;Bagging - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-15-2.png" width="672" /></p>
<pre class="r"><code>vip(rf_model, num_features = 25) + 
  ggtitle(&#39;Random Forests - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-15-3.png" width="672" /></p>
<pre class="r"><code>vip(gbm_model, num_features = 25) + 
  ggtitle(&#39;Gradient Boosting - Variable Importance Plot&#39;)</code></pre>
<p><img src="HW9_Website_files/figure-html/unnamed-chunk-15-4.png" width="672" /></p>
<p>It seems that the simple decision tree, random forests model, and gradient boosting model placed the most importance on the presence of the word “kn” and the other commonly used words. The bagging model, on the other hand places little importance on the presence of these words and instead privileges tweet length, AFINN score, and sentiments. All of the ensemble methods identified tweet length as strongly predictive of the user. All four heavily weighted anticipation sentiments and AFINN scores.</p>
</div>
<div id="vi.-results-and-discussion" class="section level1">
<h1>VI. Results and Discussion</h1>
<p>Now, I produce confusion matrices and show residual sum of squares for all tree-based methods—first evaluating their performance on the training set and then on the test set. Note again that a Tiktok tweet is encoded as 1, and a Facebook tweet is encoded as 0. The code is shown for the first matrix but not for subsequent ones for the sake of elegance.</p>
<div id="training-set-performance" class="section level2">
<h2>Training Set Performance</h2>
<p><strong>Simple Decision Tree - Training Set:</strong></p>
<pre><code>## [1] 1 0
## Levels: 0 1</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2300  108
##          1  247 2459
##                                           
##                Accuracy : 0.9306          
##                  95% CI : (0.9233, 0.9374)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.8611          
##                                           
##  Mcnemar&#39;s Test P-Value : 2.402e-13       
##                                           
##               Precision : 0.9551          
##                  Recall : 0.9030          
##                      F1 : 0.9284          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4497          
##    Detection Prevalence : 0.4709          
##       Balanced Accuracy : 0.9305          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Bagging Method - Training Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2512    3
##          1   35 2564
##                                           
##                Accuracy : 0.9926          
##                  95% CI : (0.9898, 0.9947)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9851          
##                                           
##  Mcnemar&#39;s Test P-Value : 4.934e-07       
##                                           
##               Precision : 0.9988          
##                  Recall : 0.9863          
##                      F1 : 0.9925          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4912          
##    Detection Prevalence : 0.4918          
##       Balanced Accuracy : 0.9925          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Random Forests Method - Training Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2504    3
##          1   43 2564
##                                          
##                Accuracy : 0.991          
##                  95% CI : (0.988, 0.9934)
##     No Information Rate : 0.502          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.982          
##                                          
##  Mcnemar&#39;s Test P-Value : 8.912e-09      
##                                          
##               Precision : 0.9988         
##                  Recall : 0.9831         
##                      F1 : 0.9909         
##              Prevalence : 0.4980         
##          Detection Rate : 0.4896         
##    Detection Prevalence : 0.4902         
##       Balanced Accuracy : 0.9910         
##                                          
##        &#39;Positive&#39; Class : 0              
## </code></pre>
<p><strong>Gradient Boosting Method - Training Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 2374   55
##          1  173 2512
##                                           
##                Accuracy : 0.9554          
##                  95% CI : (0.9494, 0.9609)
##     No Information Rate : 0.502           
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9108          
##                                           
##  Mcnemar&#39;s Test P-Value : 9.297e-15       
##                                           
##               Precision : 0.9774          
##                  Recall : 0.9321          
##                      F1 : 0.9542          
##              Prevalence : 0.4980          
##          Detection Rate : 0.4642          
##    Detection Prevalence : 0.4750          
##       Balanced Accuracy : 0.9553          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Performance Summary and RSS</strong></p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
total_errors
</th>
<th style="text-align:right;">
accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Simple
</td>
<td style="text-align:right;">
355
</td>
<td style="text-align:right;">
0.9305827
</td>
</tr>
<tr>
<td style="text-align:left;">
Bagging
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
0.9925694
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forests
</td>
<td style="text-align:right;">
46
</td>
<td style="text-align:right;">
0.9910051
</td>
</tr>
<tr>
<td style="text-align:left;">
Gradient Boosting
</td>
<td style="text-align:right;">
228
</td>
<td style="text-align:right;">
0.9554165
</td>
</tr>
</tbody>
</table>
<p>The rankings for accuracy on the training set are as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Bagging method</p></li>
<li><p>Random forests</p></li>
<li><p>Gradient boosting method</p></li>
<li><p>Simple decision tree</p></li>
</ol>
<p>The bagging and random forests methods achieved impressive accuracy on the training set, both able to correctly classify more than 99% of the tweets.</p>
<p>We show the residual sum of squares for all four models on the training set below.</p>
<pre class="r"><code>rss_simple_train &lt;- sum((actual_train-simple_pred_train)^2)
rss_bagging_train &lt;- sum((actual_train-bagging_pred_train)^2)
rss_rf_train &lt;- sum((actual_train-rf_pred_train)^2)
rss_gb_train &lt;- sum((actual_train-gb_pred_train)^2)

cat(paste0(&quot;Residual Sum of Squares on Training Set\n&quot;,
           &quot;\nSimple decision tree: &quot;, round(rss_simple_train, 2), 
           &quot;\nBagging model: &quot;, round(rss_bagging_train, 2), 
           &quot;\nRandom forests model: &quot;, round(rss_rf_train, 2), 
           &quot;\nGradient boosting model: &quot;, round(rss_gb_train, 2)))</code></pre>
<pre><code>## Residual Sum of Squares on Training Set
## 
## Simple decision tree: 299.52
## Bagging model: 56.46
## Random forests model: 46
## Gradient boosting model: 187.74</code></pre>
<p>The random forests method had the lowest RSS, despite the bagging method achieving higher predictive accuracy on the training set. The bagging method performed second best, followed by the gradient boosting method and simple decision tree.</p>
<p>Now, we show confusion matrices for the test set.</p>
</div>
<div id="test-set-performance" class="section level2">
<h2>Test Set Performance</h2>
<p><strong>Simple Decision Tree - Test Set:</strong></p>
<pre class="r"><code>actual_test &lt;- test$user

simple_pred_test &lt;- 
  predict(simple_model, newdata = test) %&gt;% 
  as_tibble() %&gt;% 
  select(2) %&gt;% 
  unlist() %&gt;% 
  as.vector()

simple_test_confusion &lt;- 
  confusionMatrix(data = factor(round(simple_pred_test)),
                  reference = factor(actual_test), mode = &quot;prec_recall&quot;)

simple_test_errors &lt;- 
  simple_test_confusion$table[2] +
  simple_test_confusion$table[3]

simple_test_accuracy &lt;-
  as.numeric(simple_test_confusion$overall[1])

simple_test_confusion</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 583  23
##          1  68 604
##                                           
##                Accuracy : 0.9288          
##                  95% CI : (0.9133, 0.9423)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.8577          
##                                           
##  Mcnemar&#39;s Test P-Value : 3.979e-06       
##                                           
##               Precision : 0.9620          
##                  Recall : 0.8955          
##                      F1 : 0.9276          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4562          
##    Detection Prevalence : 0.4742          
##       Balanced Accuracy : 0.9294          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Bagging Method - Test Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 603  30
##          1  48 597
##                                           
##                Accuracy : 0.939           
##                  95% CI : (0.9244, 0.9515)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt; 2e-16         
##                                           
##                   Kappa : 0.878           
##                                           
##  Mcnemar&#39;s Test P-Value : 0.05425         
##                                           
##               Precision : 0.9526          
##                  Recall : 0.9263          
##                      F1 : 0.9393          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4718          
##    Detection Prevalence : 0.4953          
##       Balanced Accuracy : 0.9392          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Random Forests Method - Test Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 602  27
##          1  49 600
##                                           
##                Accuracy : 0.9405          
##                  95% CI : (0.9261, 0.9529)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.8811          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.016           
##                                           
##               Precision : 0.9571          
##                  Recall : 0.9247          
##                      F1 : 0.9406          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4710          
##    Detection Prevalence : 0.4922          
##       Balanced Accuracy : 0.9408          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Gradient Boosting Method - Test Set:</strong></p>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 596  22
##          1  55 605
##                                           
##                Accuracy : 0.9397          
##                  95% CI : (0.9253, 0.9522)
##     No Information Rate : 0.5094          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.8796          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.0002656       
##                                           
##               Precision : 0.9644          
##                  Recall : 0.9155          
##                      F1 : 0.9393          
##              Prevalence : 0.5094          
##          Detection Rate : 0.4664          
##    Detection Prevalence : 0.4836          
##       Balanced Accuracy : 0.9402          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p><strong>Performance Summary and RSS</strong></p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
type
</th>
<th style="text-align:right;">
total_errors
</th>
<th style="text-align:right;">
accuracy
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Simple
</td>
<td style="text-align:right;">
91
</td>
<td style="text-align:right;">
0.9287950
</td>
</tr>
<tr>
<td style="text-align:left;">
Bagging
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:right;">
0.9389671
</td>
</tr>
<tr>
<td style="text-align:left;">
Random Forests
</td>
<td style="text-align:right;">
76
</td>
<td style="text-align:right;">
0.9405321
</td>
</tr>
<tr>
<td style="text-align:left;">
Gradient Boosting
</td>
<td style="text-align:right;">
77
</td>
<td style="text-align:right;">
0.9397496
</td>
</tr>
</tbody>
</table>
<p>The rankings for accuracy on the test set are as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Gradient boosting method</p></li>
<li><p>Random forests method</p></li>
<li><p>Bagging method</p></li>
<li><p>Simple decision tree</p></li>
</ol>
<p>It is worth noting though, that the differences in accuracy between the first three is incredibly small, so perhaps the tie may be broken using RSS with respect to the test set:</p>
<pre class="r"><code>rss_simple_test &lt;- sum((actual_test-simple_pred_test)^2)
rss_bagging_test &lt;- sum((actual_test-bagging_pred_test)^2)
rss_rf_test &lt;- sum((actual_test-rf_pred_test)^2)
rss_gb_test &lt;- sum((actual_test-gb_pred_test)^2)

cat(paste0(&quot;Residual Sum of Squares on Test Set\n&quot;,
           &quot;\nSimple decision tree: &quot;, round(rss_simple_test, 2), 
           &quot;\nBagging model: &quot;, round(rss_bagging_test, 2), 
           &quot;\nRandom forests model: &quot;, round(rss_rf_test, 2), 
           &quot;\nGradient boosting model: &quot;, round(rss_gb_test, 2)))</code></pre>
<pre><code>## Residual Sum of Squares on Test Set
## 
## Simple decision tree: 77.23
## Bagging model: 58.6
## Random forests model: 76
## Gradient boosting model: 60.05</code></pre>
<p>The bagging model had the lowest RSS on the test set even though it was only second best for the training set. The gradient boosting model had the second lowest RSS, followed by random forests and the simple decision tree.</p>
<p>In sum, it seems that the best model would be either the bagging model or the gradient boosting model, but this is nitpicking because all of the ensemble methods performed very well, with accuracy scores above 93%.</p>
</div>
</div>
<div id="vii.-conclusion" class="section level1">
<h1>VII. Conclusion</h1>
<p>Looking at the analyses, it seems that the Facebook and TikTok accounts have systematically different Twitter presences. Facebook seems to respond more frequently to user fears, which are associated with words such as “secure” and “trust.” Whereas, TikTok focuses on generating excitement and offer prize giveaways, which is associated with “anticipation” words such as “winning” and “tomorrow.” Differences in tweet length also possibly reflect on the preferences of the target audience; TikTok users are younger and less likely to consume written information (it is a video platform, after all), and the opposite is true for Facebook. In sum, our predictive endeavor was successful, and we unveiled a number of useful insights from it.</p>
</div>
<div id="viii.-contributions" class="section level1">
<h1>VIII. Contributions</h1>
<p>As the authorship indicates, four teammates contributed to this analysis: Ammar Plumber, Elaina Lin, Kim Nguyen, and Meghan Aines.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
